- name: Create lab environment
  hosts: localhost
  connection: local
  tasks:
  - name: Generate SSH keys
    openssh_keypair:
      path: ~/.ssh/lab_rsa
  - name: Copy SSH config to localhost
    copy:
      src: ssh.config/ssh.config
      dest: ~/.ssh/config
  - name: Get my public IP
    ipify_facts:
      timeout: 20
    delegate_to: localhost
  - name: output cloudshell public_ip
    debug: msg="my cloudshell public ip is {{ ipify_public_ip }}"
  - name: Terraform apply
    terraform:
      lock: no
      force_init: true
      variables:
        cloudshell_public_ip: "{{ ipify_public_ip }}"
      project_path: './'
      state: present
  - name: Configure local alias
    blockinfile:
      path: ~/.bashrc
      state: present
      block: |
        alias bastion='ssh -i ~/.ssh/lab_rsa azureadmin@`terraform output -raw bastion_ip`'
        alias tunnel-0='ssh -L 8080:10.30.0.6:9664 -i ~/.ssh/lab_rsa azureadmin@`terraform output -raw bastion_ip`'
        alias tunnel-1='ssh -L 8080:10.30.0.7:9664 -i ~/.ssh/lab_rsa azureadmin@`terraform output -raw bastion_ip`'
        ANSIBLE_STDOUT_CALLBACK=debug
  - name: Waiting VMs up and running
    pause:
      prompt: "Pause 15 seconds.."
      seconds: 15
  - name: Get bastion ip
    shell: terraform refresh; . ~/.bashrc

- name: Push SSH key to bastion
  hosts: bastion
  remote_user: azureadmin
  become: yes
  tasks:
  - name: Push SSH azureadmin private key
    copy:
      src: ~/.ssh/lab_rsa
      dest: /home/azureadmin/.ssh/id_rsa
      mode: '0600'
      owner: azureadmin
      group: users
  - name: Configure etc hosts
    blockinfile:
      path: /etc/hosts
      state: present
      block: |
        # IP address of the load balancer frontend configuration for NFS
        10.30.0.4 nw1-nfs
        10.30.0.5 nw2-nfs
        # NFS cluster nodes
        10.30.0.6 nfs-0
        10.30.0.7 nfs-1


- name: Push SSH config
  hosts: all
  debugger: on_failed
  remote_user: azureadmin
  tasks:
  - name: Push SSH config to all VMs
    blockinfile:
      path: /home/azureadmin/.ssh/config
      owner: azureadmin
      group: users
      create: yes
      block: |
        Host *
          StrictHostKeyChecking no
          UserKnownHostsFile=/dev/null

- name: Push root SSH keys to nfs hosts
  hosts: nfs
  remote_user: azureadmin
  become: yes
  tasks:
  - name: Create /root/.ssh directory
    file:
      path: /root/.ssh/
      state: directory
      mode: '0700'
  - name: Copy root private key
    copy:
      src: ~/.ssh/lab_rsa
      dest: /root/.ssh/id_rsa
      owner: root
      group: root
      mode: '0600'
  - name: Copy root public key to authorized keys
    copy:
      src: ~/.ssh/lab_rsa.pub
      dest: /root/.ssh/authorized_keys
      owner: root
      group: root
      mode: '0644'
  - name: Copy root public key to .ssh
    copy:
      src: ~/.ssh/lab_rsa.pub
      dest: /root/.ssh/id_rsa.pub
      owner: root
      group: root
      mode: '0644'

- name: Configure SBD on Bastion VM
  hosts: bastion
  remote_user: azureadmin
  become: yes
  tasks:
  - name: Install targetcli-fb
    zypper:
      name: targetcli-fb
      state: present
  - name: Enable targetcli
    service:
      name: targetcli
      enabled: yes
  - name: Start targetcli
    service:
      name: targetcli
      state: started
  - name: Create /sbd directory
    file:
      path: /sbd
      state: directory
  - name: Create iSCSI devices on iSCSI target server
    script: sbd.config/targetcli.config
    args:
      creates: /etc/target/saveconfig.json

- name: Configure SBD fencing on both NFS nodes
  hosts: nfs
  remote_user: azureadmin
  become: yes
  tasks:
  - name: Enable iscsid
    service:
      name: iscsid
      enabled: yes
  - name: Enable iscsi
    service:
      name: iscsi
      enabled: yes
  - name: Enable sbd
    command: sudo systemctl enable sbd
  - name: Change initiator name
    copy:
      src: nfs.config/{{ initiator_config }}
      dest: /etc/iscsi/initiatorname.iscsi
      owner: root
      group: root
  - name: Start iscsid
    service:
      name: iscsid
      state: restarted
  - name: Start iscsi
    service:
      name: iscsi
      state: restarted
      
  - name: Connect to iSCSI devices
    shell: |
      sudo sleep 10
      sudo iscsiadm -m discovery --type=st --portal=10.30.0.100:3260   
      sudo iscsiadm -m node -T iqn.2006-04.nfs.local:nfs --login --portal=10.30.0.100:3260
      sudo iscsiadm -m node -p 10.30.0.100:3260 -T iqn.2006-04.nfs.local:nfs --op=update --name=node.startup --value=automatic

  - name: Retrieve IDs of iSCSI devices, Create the SBD devices, Adapt the SBD config
    script: nfs.config/nfs.sbd.config.py
    args:
      executable: python
      creates: /etc/delete.to.retry.nfs.sbd.config.py
  
  - name: Create the softdog configuration file
    lineinfile:
      dest: /etc/modules-load.d/softdog.conf
      line: "softdog"
      state: present
      create: true 

  - name: Load softdog module
    command: sudo modprobe -v softdog

  - name: Install socat
    zypper:
      name: socat
      state: present
  - name: Install resource-agents
    zypper:
      name: resource-agents
      state: present
  - name: Configure systemd Task Max value
    lineinfile:
      dest: /etc/systemd/system.conf
      line: "DefaultTasksMax=4096"
      state: present 
  
  - name: Reload daemon-reload
    command: sudo systemctl daemon-reload 
  - name: Configure systemd
    blockinfile:
      path: /etc/sysctl.conf
      block: |
        vm.dirty_bytes = 629145600
        vm.dirty_background_bytes = 314572800

  - name: Configure interface for cloud
    lineinfile:
      path: /etc/sysconfig/network/ifcfg-eth0
      regexp: '^CLOUD_NETCONFIG_MANAGE='
      line: CLOUD_NETCONFIG_MANAGE='no'

  - name: Install fence-agents
    zypper:
      name: fence-agents
      state: present

- name: Initialize cluster on NFS node0
  hosts: nfs-node0
  remote_user: azureadmin
  become: yes
  tasks:
  - name: Stop corosync
    command: sudo service corosync stop
    args:
      creates: /etc/corosync/corosync.conf
  - name: Stop pacemaker
    command: sudo service pacemaker stop
    args:
      creates: /etc/corosync/corosync.conf
  - name: Configure ha-cluster-init
    expect:
      command: sudo ha-cluster-init -u
      echo: yes
      creates: /etc/corosync/corosync.conf
      responses:
        "Do you want to continue anyway (y/n)?": "y"
        "/root/.ssh/id_rsa already exists - overwrite (y/n)?": "n"
        '  Address for ring0': ""
        "  Port for ring0": ""
        "Do you wish to use SBD (y/n)?": "y"
        "Do you wish to configure a virtual IP address (y/n)?": "n"
        'csync2 is already configured - overwrite (y/n)?': 'y'
        '/etc/corosync/authkey already exists - overwrite (y/n)?': 'y'        
        '/etc/pacemaker/authkey already exists - overwrite (y/n)?': 'y'
        'SBD is already configured to use': 'n'
      timeout: 300
  - name: Update corosync config
    copy:
      src: nfs.config/nfs.corosync.config
      dest: /etc/corosync/corosync.conf
  - name: Restart corosync
    command: sudo service corosync restart

- name: Join NFS node1 to cluster
  hosts: nfs-node1
  remote_user: azureadmin
  become: yes
  tasks:
  - name: Stop corosync
    command: sudo service corosync stop
    args:
      creates: /etc/corosync/corosync.conf
  - name: Stop pacemaker
    command: sudo service pacemaker stop
    args:
      creates: /etc/corosync/corosync.conf
  - name: Join node to cluster
    expect:
      command: sudo ha-cluster-join
      echo: yes
      creates: /etc/corosync/corosync.conf
      responses:
        'Do you want to continue anyway (y/n)?': 'y'
        '  IP address or hostname of existing node': '10.30.0.6'
        '/root/.ssh/id_rsa already exists - overwrite (y/n)?': 'n'
        '  Address for ring0': ''
      timeout: 300

- name: Configure STONITH with SBD on cluster
  hosts: nfs-node0
  remote_user: azureadmin
  become: yes
  tasks:
  - name: Configure stonith device
    script: nfs.config/nfs.stonith.config.sh
    args:
      creates: /etc/delete.to.retry.nfs.stonith.config.sh

- name: Install and configure DRBD on NFS nodes
  hosts: nfs
  remote_user: azureadmin
  become: yes
  tasks:
  - name: Configure etc hosts
    blockinfile:
      path: /etc/hosts
      state: present
      block: |
        # IP address of the first cluster node
        10.30.0.6 nfs-0
        # IP address of the second cluster node
        10.30.0.7 nfs-1
        # IP address of the load balancer frontend configuration for NFS
        10.30.0.4 nw1-nfs
        10.30.0.5 nw2-nfs
  - name: Create /srv/nfs/ directory if it does not exist
    file:
      path: /srv/nfs/
      state: directory
  - name: Create root NFS export entry
    lineinfile:
      dest: /etc/exports
      line: "/srv/nfs/ *(rw,no_root_squash,fsid=0)"
      state: present 
  - name: Install drbd
    zypper:
      name: drbd
      state: present
  - name: Install drbd-kmp-default
    zypper:
      name: drbd-kmp-default
      state: present
  - name: Install drbd-utils
    zypper:
      name: drbd-utils
      state: present
  - name: Create LVM configurations
    script: nfs.config/nfs.lvm.config.py
    args:
      executable: python
      creates: /etc/delete.to.retry.nfs.lvm.config.py
  - name: Configure DRBD
    copy:
      src: nfs.config/nfs.drdb.config
      dest: /etc/drbd.d/global_common.conf
  - name: Create the NFS drbd device NW1
    copy:
      src: nfs.config/nw1.drdb.config
      dest: /etc/drbd.d/NW1-nfs.res
  - name: Create the NFS drbd device NW2
    copy:
      src: nfs.config/nw2.drdb.config
      dest: /etc/drbd.d/NW2-nfs.res
  - name: Create the drbd device and start it
    script: nfs.config/drdb.create.config.sh
    args:
      creates: /etc/delete.to.retry.drdb.create.config.sh

- name: Configure DRBD in the cluster
  hosts: nfs-node0
  remote_user: azureadmin
  become: yes
  tasks:
  - name: Create the drbd devices 
    script: nfs.config/nfs.drdb2.config.sh
    args:
      creates: /etc/delete.to.retry.drdb.create
  - name: Add NFS drbd devices for SAP system NW1 to the cluster configuration
    script: nfs.config/drdb.cluster1.config.sh
    args:
      creates: /etc/delete.to.retry.drdb.cluster1.config.sh
  - name: Add NFS drbd devices for SAP system NW2 to the cluster configuration
    script: nfs.config/drdb.cluster2.config.sh
    args:
      creates: /etc/delete.to.retry.drdb.cluster2.config.sh
  - name: Disable maintenance mode
    command: sudo crm configure property maintenance-mode=false
  - name: Cleanup Cluster and split nfs services on different nodes
    command: "{{ item }}"
    with_items:
      - sudo crm resource cleanup g-NW1_nfs
      - sudo crm resource cleanup g-NW2_nfs
      - sudo crm resource move g-NW2_nfs nfs-1
